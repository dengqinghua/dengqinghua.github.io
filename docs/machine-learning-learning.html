<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>机器学习</title>

  <!-- Begin Jekyll SEO tag v2.6.1 -->
<meta name="generator" content="Jekyll v3.8.7" />
<meta property="og:title" content="机器学习" />
<meta name="author" content="dengqinghua" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="机器学习的思考故事" />
<meta property="og:description" content="机器学习的思考故事" />
<link rel="canonical" href="https://dengqinghua.github.io/machine-learning-learning.html" />
<meta property="og:url" content="https://dengqinghua.github.io/machine-learning-learning.html" />
<meta property="og:site_name" content="Dengqinghua.42" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-01-04T00:00:00+08:00" />
<script type="application/ld+json">
{"dateModified":"2022-01-04T00:00:00+08:00","datePublished":"2022-01-04T00:00:00+08:00","url":"https://dengqinghua.github.io/machine-learning-learning.html","author":{"@type":"Person","name":"dengqinghua"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://dengqinghua.github.io/machine-learning-learning.html"},"@type":"BlogPosting","description":"机器学习的思考故事","headline":"机器学习","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  <link type="application/atom+xml" rel="alternate" href="https://dengqinghua.github.io/feed.xml" title="Dengqinghua.42" />

  <link rel="shortcut icon" type="image/x-icon" href="/logo.png" />
  <link rel="stylesheet" href="/assets/css/main.css" />
  <link rel="stylesheet" href="/assets/css/zoom.css" />
  <link rel="stylesheet" href="/assets/css/gitalk.css" />
  <link rel="stylesheet" href="/assets/css/syntax.css" />
  <script src="assets/js/mermaid.min.js"></script>
  <script src="assets/js/jquery.min.js"></script>
  <script src="assets/js/zoom.min.js"></script>
  <script src="assets/js/zoom_init.js"></script>
  <script src="assets/js/mermaid_config.js"></script>
  <script src="assets/js/gitalk.min.js"></script>
  <script src="assets/js/gitalk_init.js"></script>
  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <script src="assets/js/mathjax_init.js"></script>
</head>
<body a="light">
    <main class="page-content" aria-label="Content">
      <div class="w">
        <a href="/">..</a>

<article>
  <p class="post-meta">
    <time datetime="2022-01-04 00:00:00 +0800">2022-01-04</time>
  </p>
  
  <h1>机器学习</h1>

  <h2 id="机器学习的思考故事">机器学习的思考故事</h2>

<h2 id="吴恩达">吴恩达</h2>
<h3 id="machine-learning">Machine Learning</h3>
<h4 id="week1">Week1</h4>
<blockquote>
  <ol>
    <li>学习算法，模拟人类大脑的学习方式</li>
    <li>Machine Learning defined by Arthur Samuel(1959), ability to learn without explictly programmed</li>
    <li>E, T, P</li>
  </ol>
</blockquote>

<ul>
  <li>Supervised Learning
    <ul>
      <li>给出现有的结果集(Right Answers for each example)，去推导因果关系</li>
      <li>Regression Problem, 回归问题, 预测的是 连续的数据</li>
      <li>Classification Problem, 分类问题，预测的是 有限的取值</li>
      <li>Infinite number of features, 无限的特征和属性</li>
    </ul>
  </li>
  <li>UnSupervised Learning
    <ul>
      <li>No labels</li>
      <li>clusters，聚类</li>
    </ul>
  </li>
  <li>工具: octave</li>
  <li>鸡尾酒会效应(cocktail-party-effect), 人们可以在嘈杂的环境进行交谈，忽略掉背景噪声而听到对方的谈话。属于 图形-背景现象 的听觉版本</li>
</ul>

<p>线性代数算法</p>

<ul>
  <li>Training Set</li>
  <li>m: numbers of training examples</li>
  <li>x’s = input / features</li>
  <li>y’s = output / target variable</li>
  <li>
<script type="math/tex">(x, y)</script> training example</li>
  <li>
<script type="math/tex">(x^{(i)}, y^{(i)})</script> case of i, i 为 index</li>
  <li>h hypothesis, 假设 <script type="math/tex">y = h(x)</script>
</li>
  <li>linare regression with one variable <script type="math/tex">h_\theta = \theta_0 + \theta_1 x</script>
</li>
  <li>univariate linear regression 单变量线性回归</li>
  <li>目标, 找到最小值: <script type="math/tex">minimize_{\theta_0\theta_1} {1 \over2m} \sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})^2</script>
</li>
  <li>Cost Function 定义为 <script type="math/tex">J(\theta_0, \theta_1) = {1 \over2m} \sum_{i=1}^m(\hat y_i - y^{(i)})^2 = {1 \over2m} \sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})^2</script>
</li>
  <li>目标: <script type="math/tex">minimize_{\theta_0\theta_1} J(\theta_0, \theta_1)</script> 为平方差代价函数</li>
  <li>目标: <script type="math/tex">J(\theta_0, \theta_1)</script> 导数为 0 的那个 <script type="math/tex">\theta_1</script> 的值</li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Hypothesis(假设)</th>
      <th style="text-align: center">Paramemters(参数)</th>
      <th style="text-align: center">CostFunction</th>
      <th style="text-align: center">Goal</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><script type="math/tex">h_\theta = \theta_0 + \theta_1 x</script></td>
      <td style="text-align: center"><script type="math/tex">\theta_0, \theta_1</script></td>
      <td style="text-align: center"><script type="math/tex">J(\theta_0, \theta_1) = {1 \over2m} \sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})^2</script></td>
      <td style="text-align: center"><script type="math/tex">minimize_{\theta_0\theta_1} J(\theta_0, \theta_1)</script></td>
    </tr>
  </tbody>
</table>

<ul>
  <li>contour plot, 画图的软件, 见 <a href="https://www.itl.nist.gov/div898/handbook/eda/section3/contour.htm" target="_blank" rel="noopener noreferrer">这里</a> 和 <a href="https://plotly.com/javascript/contour-plots/" target="_blank" rel="noopener noreferrer">JS 版本</a>
</li>
</ul>

<p>梯度下降算法 Gradient Descent</p>

<p>每次寻找对应的 <script type="math/tex">\theta_0, \theta_1</script>，使得下面的值越来越小</p>

<script type="math/tex; mode=display">J(\theta_0, \theta_1)</script>

<p>步骤:</p>

<script type="math/tex; mode=display">\text{repeat untile convergence} \\
\\ \theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} {J(\theta_0, \theta_1)},\text{ for (j=0 and j=1)}</script>

<ul>
  <li>
<script type="math/tex">\alpha</script> 为 learning rate，越大代表下降越快。</li>
  <li>
    <p>需要同时更新 <script type="math/tex">\theta_0, \theta_1</script> 的值</p>

    <table>
      <tbody>
        <tr>
          <td><script type="math/tex">temp0 := \theta_0 - \alpha \frac{\partial}{\partial \theta_0} {J(\theta_0, \theta_1)}</script></td>
        </tr>
        <tr>
          <td><script type="math/tex">temp1 := \theta_1 - \alpha \frac{\partial}{\partial \theta_1} {J(\theta_0, \theta_1)}</script></td>
        </tr>
        <tr>
          <td><script type="math/tex">\theta_0 := temp0</script></td>
        </tr>
        <tr>
          <td><script type="math/tex">\theta_1 := temp1</script></td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<p><img src="assets/images/gradient-descent.png" alt="gradient-descent"></p>

<ul>
  <li>通过斜率的变化，来动态调整参数的值，使得达到收敛(converge)的点, 也就是图中的最低点的位置</li>
  <li>偏离 diverge</li>
  <li>
<script type="math/tex">\alpha</script> 很小，则找到具体的点会比较慢, 过大，则容易错过最优点</li>
  <li>
    <p>根据导数的值的大小，动态地调整 <script type="math/tex">\alpha</script> 的值</p>
  </li>
  <li>convex 凸函数(bowl shaped function)</li>
</ul>

<p>第一个机器学习的算法</p>

<blockquote>

  <p>在这之前，需要复习几个<a href="https://baike.baidu.com/item/%E6%B1%82%E5%AF%BC/1063861" target="_blank" rel="noopener noreferrer">求导法则</a>。有助于理解下面的公式计算</p>

  <p>假设 <script type="math/tex">u(x),v(x)</script> 均可导, 则</p>

  <script type="math/tex; mode=display">% <![CDATA[
\begin{align}
  (u(x) \pm v(x))^{'} &= u^{'}(x) \pm v^{'}(x) \\
  (u(x)v(x))^{'} &= u^{'}(x)v(x) + u(x)v^{'}(x) \\
  (u(v(x)))^{'} &= u^{'}(x)v(x) \\
  ({u(x) \over v(x)})^{'} &= \frac{u^{'}(x)v(x) - u(x)v^{'}(x)}{v^2(x)} \\
  (cu(x))^{'} &= cu^{'}(x) \text{, c 为常数} \\
  \end{align} %]]></script>
</blockquote>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Hypothesis(假设)</th>
      <th style="text-align: center">Paramemters(参数)</th>
      <th style="text-align: center">CostFunction</th>
      <th style="text-align: center">Goal</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><script type="math/tex">h_\theta = \theta_0 + \theta_1 x</script></td>
      <td style="text-align: center"><script type="math/tex">\theta_0, \theta_1</script></td>
      <td style="text-align: center"><script type="math/tex">J(\theta_0, \theta_1) = {1 \over2m} \sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})^2</script></td>
      <td style="text-align: center"><script type="math/tex">minimize_{\theta_0\theta_1} J(\theta_0, \theta_1)</script></td>
    </tr>
  </tbody>
</table>

<ol>
  <li>
    <p>对 CostFunction 求导数</p>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{align}
 \frac{\partial}{\partial \theta_j} J(\theta) &= {1 \over m} \sum_{i=1}^m((h_\theta(x^{(i)}) - y^{(i)})^{'}(h_\theta(x^{(i)}) - y^{(i)})) \\
        &= {1 \over m} \sum_{i=1}^m(h^{'}_\theta(x^{(i)})(h_\theta(x^{(i)}) - y^{(i)})) \\
        &= {1 \over m} \sum_{i=1}^m(\theta_0 + \theta_1 x^{(i)})^{'}(h_\theta(x^{(i)}) - y^{(i)})) \\
 \end{align} %]]></script>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{align}
 \frac{\partial}{\partial \theta_0} J(\theta) &= {1 \over m} \sum_{i=1}^m(\theta_0 + \theta_1 x^{(i)})^{'}(h_\theta(x^{(i)}) - y^{(i)})) \\
        &= {1 \over m} \sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})) \\
        &= {1 \over m} \sum_{i=1}^m(\theta_0 + \theta_1 x^{(i)} - y^{(i)}))
 \end{align} %]]></script>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{align}
 \frac{\partial}{\partial \theta_1} J(\theta) &= {1 \over m} \sum_{i=1}^m(\theta_0 + \theta_1 x^{(i)})^{'}(h_\theta(x^{(i)}) - y^{(i)})) \\
        &= {1 \over m} \sum_{i=1}^m(x^{(i)})(\theta_0 + \theta_1 x^{(i)} - y^{(i)}))
 \end{align} %]]></script>
  </li>
  <li>
    <p>将 导数 部分代入到 梯度下降算法中</p>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{align}
 \text{repeat untile convergence...} &\{ \\
 temp0 &:= \theta_0 - {\alpha \over m} \sum_{i=1}^m(\theta_0 + \theta_1 x^{(i)} - y^{(i)})) \\
 temp1 &:= \theta_1 - {\alpha \over m} \sum_{i=1}^m(x^{(i)})(\theta_0 + \theta_1 x^{(i)} - y^{(i)})) \\
 \theta_0 &:= temp0 \\
 \theta_1 &:= temp1 \\
 &\}
 \end{align} \\ %]]></script>
  </li>
</ol>

<p>metrics 和 vectors</p>

<p>下面为一个 2 x 3 的矩阵</p>

<script type="math/tex; mode=display">% <![CDATA[
A = \begin{bmatrix}
1 & 2 & 3 \\
1 & 3 & 5 \\
\end{bmatrix} %]]></script>

<p><script type="math/tex">A_{ij}</script> 为 第 i 行，第 j 列的值，注意 i，j 都是从 1 开始的</p>

<p>vector: n x 1 的矩阵, 如下为 3维 vector</p>

<script type="math/tex; mode=display">A = \begin{bmatrix}
1 \\
2 \\
3 \\
\end{bmatrix}</script>

<ul>
  <li>矩阵的乘法: <script type="math/tex">A_{m,n} \times x_{n,1} = y_{m,1}\\
A_{m,n} \times B_{n,o} = C_{m,o}</script>
</li>
  <li>1-indexed vs 0-indexed vector</li>
  <li>大写字母表示矩阵，小写字母表示向量</li>
  <li>矩阵运算见<a href="https://baike.baidu.hk/item/%E7%9F%A9%E9%99%A3/18069" target="_blank" rel="noopener noreferrer">这里</a>
</li>
  <li>scalar 标量，raw number</li>
  <li>使用 矩阵运算，而不是 for 运算，能够更加简洁，高效地计算。
  <img src="assets/images/for-matrix-example.png" alt="for-matrix-example">
</li>
  <li>不满足交换律: <script type="math/tex">A \times B \neq B \times A</script> (not commutative)</li>
  <li>满足结合律: <script type="math/tex">(A \times B) \times C  = A \times (B \times C)</script> (associative)</li>
  <li>单位矩阵，Diagonal or Identity Matrix：<script type="math/tex">I_{n \times n}</script>
</li>
  <li>单位矩阵满足：<script type="math/tex">A_{m,n} \times I_{n,n} = I_{m,m} \times A_{m,n} = A_{m,n}</script>
</li>
  <li>逆矩阵 matrix inverse。<script type="math/tex">AA^{-1} = A^{-1}A = I</script> 则 <script type="math/tex">A^{-1}</script> 为 A 的 逆矩阵 (A为 mxm 矩阵，也就是 square matrix, 方阵)</li>
  <li>奇异矩阵 sigular matrix，没有逆矩阵的矩阵</li>
  <li>
    <p>转置矩阵 transpose matrix</p>

    <script type="math/tex; mode=display">% <![CDATA[
A = \begin{bmatrix}
1 & 2 & 3 \\
1 & 3 & 5 \\
\end{bmatrix},

A^T = \begin{bmatrix}
1 & 1 \\
2 & 3 \\
3 & 5 \\
\end{bmatrix} %]]></script>
  </li>
  <li>octave 中矩阵的操作可参考<a href="http://www.philender.com/courses/multivariate/notes/matoctave.html" target="_blank" rel="noopener noreferrer">这里</a>
</li>
</ul>

<h4 id="week2">Week2</h4>
<p>使用 vector 和 matrix 来表示 multi feature hypothesis，多维度的假设函数</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}h(\theta) &= \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \cdots + \theta_n x_n \\
&= \theta_0 x_0 + \theta_1 x_1 + \theta_2 x_2 + \cdots + \theta_n x_n \text{ for } x_0 = 1 \\
&= \begin{bmatrix}
\theta_0 & \theta_1 & \cdots & \theta_n
\end{bmatrix}
\times \begin{bmatrix}
x_0 \\
x_1 \\
\vdots \\
x_n
\end{bmatrix} \\
&= \theta^T \times x
\end{align} %]]></script>

<p>其中 <script type="math/tex">\theta</script> 和 <script type="math/tex">x</script> 均为 vector</p>

<p>使用矢量/矩阵来实现多特征梯度下降 (multi feature gradient descent)</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Hypothesis(假设)</th>
      <th style="text-align: center">Paramemters(参数)</th>
      <th style="text-align: center">CostFunction</th>
      <th style="text-align: center">Goal</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><script type="math/tex">% <![CDATA[
\begin{align}y^{(i)} &= \theta_0 x_0^{(i)} + \theta_1 x_1^{(i)} + \cdots + \theta_n x_n^{(i)} \\ &= (x^{(i)})^{T} \times \theta \\ Y &= X \times \theta \end{align} %]]></script></td>
      <td style="text-align: center"><script type="math/tex">\theta = \begin{bmatrix} \theta_0 \\ \theta_1 \\ \cdots \\ \theta_n \end{bmatrix}</script></td>
      <td style="text-align: center"><script type="math/tex">J(\theta) = {1 \over2m} \sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})^2</script></td>
      <td style="text-align: center"><script type="math/tex">minimize_{\theta} J(\theta_0, \theta_1, \cdots, \theta_n)</script></td>
    </tr>
  </tbody>
</table>

<p>Gradient Descent</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\text{repeat } &\{ \\
\theta_j &:= \theta_j - \alpha \frac{\partial}{\partial \theta_j} {J(\theta)},\text{ for } j=0, 1, \cdots, n \\
         &:= \theta_j - \alpha \sum_{i=1}^m(x^{(i)}_{j})(h_\theta - y^{(i)})) \\
\}
\end{align} %]]></script>

<ul>
  <li>
    <p>feature scaling, 将特征值进行缩放，使得图形能够更快地收敛</p>

    <p><img src="assets/images/feature-scale.png" alt="feature-scale"></p>
  </li>
  <li>使得所有的特征的值接近 <script type="math/tex">-1 \leq x \leq 1</script> 的区间，建议是差别不超过三倍</li>
  <li>Mean Normalization, 归一化 <script type="math/tex">x = \frac{ x - \mu }{s}</script>, 其中 <script type="math/tex">\mu</script> 为特征 x 的均值, s 为(最大值-最小值) 或者是标准差</li>
  <li>特征缩放不需要非常准备</li>
  <li>选择 <script type="math/tex">\alpha</script> (Learning rate) 的技巧。变小的幅度小于 <script type="math/tex">10^{-3}</script> 便可停止了</li>
  <li>Polynomial Regression 多项式回归, 如 <script type="math/tex">h_\theta = \theta_0 x_0 + \theta_1 x_1^2 + \cdots + \theta_n x_n^n</script>
</li>
  <li>Polynomial Regression 的参数的 scaling 很重要，因为数值会随着 <script type="math/tex">x^n</script> 的 n 指数型增长</li>
  <li>模型变量的选择：可以是原始变量的组合。</li>
  <li>
    <p>Normal Equation, 一次性求解出所有的 <script type="math/tex">\theta</script>，类似于解矩阵方程的思路，下面是结果。参考 <a href="https://blog.csdn.net/artprog/article/details/51172025" target="_blank" rel="noopener noreferrer">机器学习笔记03：Normal equation与梯度下降的比较</a> 其中 m 为样本数，n 为特征数 和 <a href="https://stanford.edu/~rezab/classes/cme323/S15/notes/lec11.pdf" target="_blank" rel="noopener noreferrer">复杂度分析</a></p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">Hypothesis(假设)</th>
          <th style="text-align: center">features <script type="math/tex">X_{(m, n+1)}</script>
</th>
          <th style="text-align: center">Paramemters <script type="math/tex">\theta</script>
</th>
          <th style="text-align: center">Normal Equation Answer</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><script type="math/tex">% <![CDATA[
\begin{align}y^{(i)} &= \theta_0 x_0^{(i)} + \theta_1 x_1^{(i)} + \cdots + \theta_n x_n^{(i)} \\ &= (x^{(i)})^{T} \times \theta \\ Y &= X \times \theta \end{align} %]]></script></td>
          <td style="text-align: center"><script type="math/tex">X = \begin{bmatrix} (x^{(1)})^{T} \\ (x^{(2)})^{T} \\ \cdots \\  (x^{(m)})^{T} \\ \end{bmatrix}</script></td>
          <td style="text-align: center"><script type="math/tex">\theta = \begin{bmatrix} \theta_0 \\ \theta_1 \\ \cdots \\ \theta_n \end{bmatrix}</script></td>
          <td style="text-align: center"><script type="math/tex">\theta = (X^TX)^{(-1)}X^TY</script></td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>Normal Equation 不需要做 feature scaling，但是在 n 比较大(&gt;10000)的时候比较慢，计算 <script type="math/tex">(X^TX)^{(-1)}</script> 的复杂度为 <script type="math/tex">O(n^3)</script>
</li>
  <li>什么时候矩阵是奇异矩阵，见<a href="https://byjus.com/maths/singular-matrix/#:~:text=What%20is%20Singular%20Matrix%3F,if%20its%20determinant%20is%200." target="_blank" rel="noopener noreferrer">这里</a>
</li>
</ul>

<h3 id="octave-tutorial">octave tutorial</h3>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>a = 1:0.1:2 % 从 1 到 2 以 0.1 为步长
zeros(2, 3) % 2行3列的全是0的矩阵
ones(2, 3) % 2行3列的全是1的矩阵
rand(2, 3) % 2行3列的【0到1之间的】随机数的矩阵
randn(1, 3) % 平均值为0，方差为1的高斯分布的随机矩阵
w = -6 + sqrt(10) * (randn(1, 10000)); hist(w) % 画出直方图(histogram)
eye(10) % 单位矩阵，eye 代表的是 I 的意思
pwd % 当前的路劲 类似还可以用 cd, ls 等
who % 查看当前的作用域 whos
save % 保存矩阵到对应的文件 save new.dat v
C = [3 4;2 2] % 可以省去逗号
1./C % =[1/3 1/4; 1/2 1/2]
</code></pre></div></div>

<p>plot</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>% 还可以设置坐标名称，线名称，颜色等等
t = [0:0.01:0.98]
y1 = sin(2 * pi * t)
plot(t, y1)
hold on % 画第二个图
y2 = cos(2 * pi * t)
plot(t, y2)
xlabel("横坐标")
ylabel("纵坐标")
legend("线的定义")
title("图片标题")
print -dpng % 保存文件
close % 关闭

% 定义图一和图二
figure(1); plot(t, y1)
figure(2); plot(t, y2)

% 将图分隔展示
subplot(1,2,1);plot(t, y1)
subplot(1,2,2);plot(t, y2)

% 修改中轴线
axis
</code></pre></div></div>

<p>vectorization</p>

<p>将数值运算，变成矩阵运算</p>

<h4 id="week3">Week3</h4>
<p>分类 classification</p>

<h2 id="reference">Reference</h2>
<ul>
  <li><a href="https://aistudio.baidu.com/aistudio/education/group/info/1138" target="_blank" rel="noopener noreferrer">机器学习的思考故事</a></li>
  <li><a href="https://aistudio.baidu.com/aistudio/course/introduce/1297" target="_blank" rel="noopener noreferrer">零基础实践机器学习</a></li>
  <li><a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener noreferrer">吴恩达在 coursera 的机器学习课程</a></li>
</ul>

</article>

<div id="comment"></div>
<script>
  gitalk.render('comment');
</script>



      </div>
    </main>

    
  </body>
</html>